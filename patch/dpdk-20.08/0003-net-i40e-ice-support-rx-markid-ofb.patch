diff --git a/drivers/net/i40e/i40e_rxtx.c b/drivers/net/i40e/i40e_rxtx.c
index fe7f9200c1..3ea73eb0b0 100644
--- a/drivers/net/i40e/i40e_rxtx.c
+++ b/drivers/net/i40e/i40e_rxtx.c
@@ -409,7 +409,7 @@ check_rx_burst_bulk_alloc_preconditions(__rte_unused struct i40e_rx_queue *rxq)
 #error "PMD I40E: I40E_LOOK_AHEAD must be 8\n"
 #endif
 static inline int
-i40e_rx_scan_hw_ring(struct i40e_rx_queue *rxq)
+i40e_rx_scan_hw_ring(struct i40e_rx_queue *rxq, uint8_t *ofb_id, uint8_t *ofb_data)
 {
 	volatile union i40e_rx_desc *rxdp;
 	struct i40e_rx_entry *rxep;
@@ -418,9 +418,10 @@ i40e_rx_scan_hw_ring(struct i40e_rx_queue *rxq)
 	uint64_t qword1;
 	uint32_t rx_status;
 	int32_t s[I40E_LOOK_AHEAD], nb_dd;
-	int32_t i, j, nb_rx = 0;
+	int32_t i, j, k,nb_rx = 0;
 	uint64_t pkt_flags;
 	uint32_t *ptype_tbl = rxq->vsi->adapter->ptype_tbl;
+	int ofb_magic_flag = 1;
 
 	rxdp = &rxq->rx_ring[rxq->rx_tail];
 	rxep = &rxq->sw_ring[rxq->rx_tail];
@@ -437,7 +438,7 @@ i40e_rx_scan_hw_ring(struct i40e_rx_queue *rxq)
 	 * Scan LOOK_AHEAD descriptors at a time to determine which
 	 * descriptors reference packets that are ready to be received.
 	 */
-	for (i = 0; i < RTE_PMD_I40E_RX_MAX_BURST; i+=I40E_LOOK_AHEAD,
+	for (i = 0, k = 0; i < RTE_PMD_I40E_RX_MAX_BURST; i+=I40E_LOOK_AHEAD,
 			rxdp += I40E_LOOK_AHEAD, rxep += I40E_LOOK_AHEAD) {
 		/* Read desc statuses backwards to avoid race condition */
 		for (j = I40E_LOOK_AHEAD - 1; j >= 0; j--) {
@@ -456,7 +457,7 @@ i40e_rx_scan_hw_ring(struct i40e_rx_queue *rxq)
 		nb_rx += nb_dd;
 
 		/* Translate descriptor info to mbuf parameters */
-		for (j = 0; j < nb_dd; j++) {
+		for (j = 0; j < nb_dd; j++, k++) {
 			mb = rxep[j].mbuf;
 			qword1 = rte_le_to_cpu_64(\
 				rxdp[j].wb.qword1.status_error_len);
@@ -475,8 +476,18 @@ i40e_rx_scan_hw_ring(struct i40e_rx_queue *rxq)
 			if (pkt_flags & PKT_RX_RSS_HASH)
 				mb->hash.rss = rte_le_to_cpu_32(\
 					rxdp[j].wb.qword0.hi_dword.rss);
-			if (pkt_flags & PKT_RX_FDIR)
+			if (pkt_flags & PKT_RX_FDIR) {
 				pkt_flags |= i40e_rxd_build_fdir(&rxdp[j], mb);
+                if (ofb_data) {
+				    ofb_data[k] = mb->hash.fdir.hi;
+				    if (ofb_magic_flag && ofb_data[k]) {
+                        if (ofb_id) {
+					        *ofb_id = ofb_data[k];
+                        }
+					    ofb_magic_flag = 0;
+				    }
+                }
+			}
 
 #ifdef RTE_LIBRTE_IEEE1588
 			pkt_flags |= i40e_get_iee15888_flags(mb, qword1);
@@ -569,7 +580,7 @@ i40e_rx_alloc_bufs(struct i40e_rx_queue *rxq)
 }
 
 static inline uint16_t
-rx_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
+rx_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts, uint8_t *ofb_id, uint8_t *ofb_data)
 {
 	struct i40e_rx_queue *rxq = (struct i40e_rx_queue *)rx_queue;
 	struct rte_eth_dev *dev;
@@ -581,7 +592,7 @@ rx_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 	if (rxq->rx_nb_avail)
 		return i40e_rx_fill_from_stage(rxq, rx_pkts, nb_pkts);
 
-	nb_rx = (uint16_t)i40e_rx_scan_hw_ring(rxq);
+	nb_rx = (uint16_t)i40e_rx_scan_hw_ring(rxq, ofb_id, ofb_data);
 	rxq->rx_next_avail = 0;
 	rxq->rx_nb_avail = nb_rx;
 	rxq->rx_tail = (uint16_t)(rxq->rx_tail + nb_rx);
@@ -613,9 +624,9 @@ rx_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 }
 
 static uint16_t
-i40e_recv_pkts_bulk_alloc(void *rx_queue,
+__i40e_recv_pkts_bulk_alloc(void *rx_queue,
 			  struct rte_mbuf **rx_pkts,
-			  uint16_t nb_pkts)
+			  uint16_t nb_pkts, uint8_t *ofb_id, uint8_t *ofb_data)
 {
 	uint16_t nb_rx = 0, n, count;
 
@@ -623,12 +634,13 @@ i40e_recv_pkts_bulk_alloc(void *rx_queue,
 		return 0;
 
 	if (likely(nb_pkts <= RTE_PMD_I40E_RX_MAX_BURST))
-		return rx_recv_pkts(rx_queue, rx_pkts, nb_pkts);
+		return rx_recv_pkts(rx_queue, rx_pkts, nb_pkts, ofb_id, ofb_data);
 
 	while (nb_pkts) {
 		n = RTE_MIN(nb_pkts, RTE_PMD_I40E_RX_MAX_BURST);
-		count = rx_recv_pkts(rx_queue, &rx_pkts[nb_rx], n);
+		count = rx_recv_pkts(rx_queue, &rx_pkts[nb_rx], n, ofb_id, ofb_data);
 		nb_rx = (uint16_t)(nb_rx + count);
+		ofb_data += count;
 		nb_pkts = (uint16_t)(nb_pkts - count);
 		if (count < n)
 			break;
@@ -638,14 +650,31 @@ i40e_recv_pkts_bulk_alloc(void *rx_queue,
 }
 #else
 static uint16_t
-i40e_recv_pkts_bulk_alloc(void __rte_unused *rx_queue,
+__i40e_recv_pkts_bulk_alloc(void __rte_unused *rx_queue,
 			  struct rte_mbuf __rte_unused **rx_pkts,
-			  uint16_t __rte_unused nb_pkts)
+			  uint16_t __rte_unused nb_pkts, uint8_t __rte_unused *ofb_id, uint8_t __rte_unused *ofb_data)
 {
 	return 0;
 }
 #endif /* RTE_LIBRTE_I40E_RX_ALLOW_BULK_ALLOC */
 
+static uint16_t
+i40e_recv_pkts_bulk_alloc(void *rx_queue,
+                          struct rte_mbuf **rx_pkts,
+                          uint16_t nb_pkts)
+{
+    return __i40e_recv_pkts_bulk_alloc(rx_queue, rx_pkts, nb_pkts, NULL, NULL);
+}
+
+uint16_t
+i40e_pmd_recv_pkts_bulk_alloc(void *rx_queue,
+                          struct rte_mbuf **rx_pkts,
+                          uint16_t nb_pkts, uint8_t *ofb_id, uint8_t *ofb_data)
+{
+    return __i40e_recv_pkts_bulk_alloc(rx_queue, rx_pkts, nb_pkts, ofb_id, ofb_data);
+}
+
+
 uint16_t
 i40e_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 {
diff --git a/drivers/net/i40e/i40e_rxtx.h b/drivers/net/i40e/i40e_rxtx.h
index 57d7b4160b..ad73546e31 100644
--- a/drivers/net/i40e/i40e_rxtx.h
+++ b/drivers/net/i40e/i40e_rxtx.h
@@ -195,6 +195,9 @@ int i40e_dev_tx_queue_setup(struct rte_eth_dev *dev,
 			    const struct rte_eth_txconf *tx_conf);
 void i40e_dev_rx_queue_release(void *rxq);
 void i40e_dev_tx_queue_release(void *txq);
+uint16_t i40e_pmd_recv_pkts_bulk_alloc(void *rx_queue,
+                          struct rte_mbuf **rx_pkts,
+                          uint16_t nb_pkts, uint8_t *ofb_id, uint8_t *ofb_data);
 uint16_t i40e_recv_pkts(void *rx_queue,
 			struct rte_mbuf **rx_pkts,
 			uint16_t nb_pkts);
diff --git a/drivers/net/i40e/rte_pmd_i40e.c b/drivers/net/i40e/rte_pmd_i40e.c
index 17938e7d30..0911ae399d 100644
--- a/drivers/net/i40e/rte_pmd_i40e.c
+++ b/drivers/net/i40e/rte_pmd_i40e.c
@@ -3279,3 +3279,17 @@ rte_pmd_i40e_set_switch_dev(uint16_t port_id, struct rte_eth_dev *switch_dev)
 
 	return 0;
 }
+
+uint16_t
+rte_pmd_i40e_recv_pkts_bulk_alloc(uint16_t port_id, uint16_t queue_id,
+			  struct rte_mbuf **rx_pkts, uint16_t nb_pkts, uint8_t *ofb_id, uint8_t *ofb_data)
+{
+    struct rte_eth_dev *i40e_dev;
+
+    i40e_dev = &rte_eth_devices[port_id];
+
+    if (likely(i40e_dev))
+        return i40e_pmd_recv_pkts_bulk_alloc(i40e_dev->data->rx_queues[queue_id], rx_pkts, nb_pkts, ofb_id, ofb_data);
+
+    return 0;
+}
diff --git a/drivers/net/i40e/rte_pmd_i40e.h b/drivers/net/i40e/rte_pmd_i40e.h
index fc3560c28c..1bb27c91c1 100644
--- a/drivers/net/i40e/rte_pmd_i40e.h
+++ b/drivers/net/i40e/rte_pmd_i40e.h
@@ -1130,4 +1130,9 @@ __rte_experimental
 int
 rte_pmd_i40e_set_switch_dev(uint16_t port_id, struct rte_eth_dev *switch_dev);
 
+uint16_t
+rte_pmd_i40e_recv_pkts_bulk_alloc(uint16_t port_id,
+              uint16_t queue_id, struct rte_mbuf **rx_pkts,
+			  uint16_t nb_pkts, uint8_t *ofb_id, uint8_t *ofb_data);
+
 #endif /* _PMD_I40E_H_ */
diff --git a/drivers/net/ice/ice_rxtx.c b/drivers/net/ice/ice_rxtx.c
index 2e1f06d2c0..106d51e983 100644
--- a/drivers/net/ice/ice_rxtx.c
+++ b/drivers/net/ice/ice_rxtx.c
@@ -1364,7 +1364,7 @@ ice_rxd_to_pkt_fields(struct rte_mbuf *mb,
 #error "PMD ICE: ICE_LOOK_AHEAD must be 8\n"
 #endif
 static inline int
-ice_rx_scan_hw_ring(struct ice_rx_queue *rxq)
+ice_rx_scan_hw_ring(struct ice_rx_queue *rxq, uint8_t *ofb_id, uint8_t *ofb_data)
 {
 	volatile union ice_rx_flex_desc *rxdp;
 	struct ice_rx_entry *rxep;
@@ -1372,9 +1372,10 @@ ice_rx_scan_hw_ring(struct ice_rx_queue *rxq)
 	uint16_t stat_err0;
 	uint16_t pkt_len;
 	int32_t s[ICE_LOOK_AHEAD], nb_dd;
-	int32_t i, j, nb_rx = 0;
+	int32_t i, j, k, nb_rx = 0;
 	uint64_t pkt_flags = 0;
 	uint32_t *ptype_tbl = rxq->vsi->adapter->ptype_tbl;
+    int ofb_magic_flag = 1;
 
 	rxdp = &rxq->rx_ring[rxq->rx_tail];
 	rxep = &rxq->sw_ring[rxq->rx_tail];
@@ -1389,7 +1390,7 @@ ice_rx_scan_hw_ring(struct ice_rx_queue *rxq)
 	 * Scan LOOK_AHEAD descriptors at a time to determine which
 	 * descriptors reference packets that are ready to be received.
 	 */
-	for (i = 0; i < ICE_RX_MAX_BURST; i += ICE_LOOK_AHEAD,
+	for (i=0,k=0; i < ICE_RX_MAX_BURST; i += ICE_LOOK_AHEAD,
 	     rxdp += ICE_LOOK_AHEAD, rxep += ICE_LOOK_AHEAD) {
 		/* Read desc statuses backwards to avoid race condition */
 		for (j = ICE_LOOK_AHEAD - 1; j >= 0; j--)
@@ -1404,7 +1405,7 @@ ice_rx_scan_hw_ring(struct ice_rx_queue *rxq)
 		nb_rx += nb_dd;
 
 		/* Translate descriptor info to mbuf parameters */
-		for (j = 0; j < nb_dd; j++) {
+		for (j = 0; j < nb_dd; j++, k++) {
 			mb = rxep[j].mbuf;
 			pkt_len = (rte_le_to_cpu_16(rxdp[j].wb.pkt_len) &
 				   ICE_RX_FLX_DESC_PKT_LEN_M) - rxq->crc_len;
@@ -1417,6 +1418,15 @@ ice_rx_scan_hw_ring(struct ice_rx_queue *rxq)
 				rte_le_to_cpu_16(rxdp[j].wb.ptype_flex_flags0)];
 			ice_rxd_to_vlan_tci(mb, &rxdp[j]);
 			ice_rxd_to_pkt_fields(mb, &rxdp[j]);
+            if (ofb_data && (mb->ol_flags & PKT_RX_FDIR)) {
+                ofb_data[k] = mb->hash.fdir.hi;
+                if (ofb_magic_flag && ofb_data[k]) {
+                    if (ofb_id) {
+                        *ofb_id = ofb_data[k];
+                    }
+                    ofb_magic_flag = 0;
+                }
+            }
 
 			mb->ol_flags |= pkt_flags;
 		}
@@ -1508,7 +1518,7 @@ ice_rx_alloc_bufs(struct ice_rx_queue *rxq)
 }
 
 static inline uint16_t
-rx_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
+rx_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts, uint8_t *ofb_id, uint8_t *ofb_data)
 {
 	struct ice_rx_queue *rxq = (struct ice_rx_queue *)rx_queue;
 	uint16_t nb_rx = 0;
@@ -1520,7 +1530,7 @@ rx_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 	if (rxq->rx_nb_avail)
 		return ice_rx_fill_from_stage(rxq, rx_pkts, nb_pkts);
 
-	nb_rx = (uint16_t)ice_rx_scan_hw_ring(rxq);
+	nb_rx = (uint16_t)ice_rx_scan_hw_ring(rxq, ofb_id, ofb_data);
 	rxq->rx_next_avail = 0;
 	rxq->rx_nb_avail = nb_rx;
 	rxq->rx_tail = (uint16_t)(rxq->rx_tail + nb_rx);
@@ -1554,9 +1564,9 @@ rx_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 }
 
 static uint16_t
-ice_recv_pkts_bulk_alloc(void *rx_queue,
+__ice_recv_pkts_bulk_alloc(void *rx_queue,
 			 struct rte_mbuf **rx_pkts,
-			 uint16_t nb_pkts)
+			 uint16_t nb_pkts, uint8_t *ofb_id, uint8_t *ofb_data)
 {
 	uint16_t nb_rx = 0;
 	uint16_t n;
@@ -1566,11 +1576,11 @@ ice_recv_pkts_bulk_alloc(void *rx_queue,
 		return nb_rx;
 
 	if (likely(nb_pkts <= ICE_RX_MAX_BURST))
-		return rx_recv_pkts(rx_queue, rx_pkts, nb_pkts);
+		return rx_recv_pkts(rx_queue, rx_pkts, nb_pkts, ofb_id, ofb_data);
 
 	while (nb_pkts) {
 		n = RTE_MIN(nb_pkts, ICE_RX_MAX_BURST);
-		count = rx_recv_pkts(rx_queue, &rx_pkts[nb_rx], n);
+		count = rx_recv_pkts(rx_queue, &rx_pkts[nb_rx], n, ofb_id, ofb_data);
 		nb_rx = (uint16_t)(nb_rx + count);
 		nb_pkts = (uint16_t)(nb_pkts - count);
 		if (count < n)
@@ -1580,6 +1590,37 @@ ice_recv_pkts_bulk_alloc(void *rx_queue,
 	return nb_rx;
 }
 
+static uint16_t
+ice_recv_pkts_bulk_alloc(void *rx_queue,
+            struct rte_mbuf **rx_pkts,
+            uint16_t nb_pkts)
+{
+    return __ice_recv_pkts_bulk_alloc(rx_queue, rx_pkts, nb_pkts, NULL, NULL);
+}
+
+static inline uint16_t
+ice_pmd_recv_pkts_bulk_alloc(void *rx_queue,
+                          struct rte_mbuf **rx_pkts,
+                          uint16_t nb_pkts, uint8_t *ofb_id, uint8_t *ofb_data)
+{
+    return __ice_recv_pkts_bulk_alloc(rx_queue, rx_pkts, nb_pkts, ofb_id, ofb_data);
+}
+
+uint16_t
+rte_pmd_ice_recv_pkts_bulk_alloc(uint16_t port_id, uint16_t queue_id,
+            struct rte_mbuf **rx_pkts, uint16_t nb_pkts, uint8_t *ofb_id, uint8_t *ofb_data)
+{
+    struct rte_eth_dev *ice_dev;
+
+    ice_dev = &rte_eth_devices[port_id];
+
+    if (likely(ice_dev))
+      return ice_pmd_recv_pkts_bulk_alloc(ice_dev->data->rx_queues[queue_id], rx_pkts, nb_pkts, ofb_id, ofb_data);
+
+    return 0;
+}
+
+
 static uint16_t
 ice_recv_scattered_pkts(void *rx_queue,
 			struct rte_mbuf **rx_pkts,
diff --git a/drivers/net/ice/rte_pmd_ice.h b/drivers/net/ice/rte_pmd_ice.h
index e254db0534..5c38cff8d5 100644
--- a/drivers/net/ice/rte_pmd_ice.h
+++ b/drivers/net/ice/rte_pmd_ice.h
@@ -224,6 +224,9 @@ rte_net_ice_dump_proto_xtr_metadata(struct rte_mbuf *m)
 		       data.tcp.fin ? "F" : "");
 }
 
+uint16_t rte_pmd_ice_recv_pkts_bulk_alloc(uint16_t port_id, uint16_t queue_id,
+			  struct rte_mbuf **rx_pkts, uint16_t nb_pkts, uint8_t *ofb_id, uint8_t *ofb_data);
+
 #ifdef __cplusplus
 }
 #endif
